Metadata-Version: 2.4
Name: pantheraml_zoo
Version: 2025.6.8
Summary: PantheraML Zoo - Production-ready multi-GPU/TPU training utilities with monitoring, error handling, and performance optimization
Author: Unsloth AI team
Author-email: info@unsloth.ai
Maintainer-email: Daniel Han <danielhanchen@gmail.com>, Michael Han <info@unsloth.ai>
License: LGPL-3.0-or-later
Project-URL: homepage, http://www.unsloth.ai
Project-URL: documentation, https://github.com/unslothai/unsloth
Project-URL: repository, https://github.com/PantheraAIML/PantheraML-Zoo
Keywords: ai,llm,training,multi-gpu,tpu,distributed,production,monitoring
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: <3.13,>=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch<=2.7.0
Requires-Dist: triton; platform_system == "Linux"
Requires-Dist: triton_windows; platform_system == "Windows"
Requires-Dist: packaging>=24.1
Requires-Dist: tyro
Requires-Dist: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,>=4.51.3
Requires-Dist: datasets>=3.4.1
Requires-Dist: sentencepiece>=0.2.0
Requires-Dist: tqdm
Requires-Dist: psutil
Requires-Dist: wheel>=0.42.0
Requires-Dist: numpy
Requires-Dist: accelerate>=0.34.1
Requires-Dist: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9
Requires-Dist: peft!=0.11.0,>=0.7.1
Requires-Dist: protobuf<4.0.0
Requires-Dist: huggingface_hub>=0.30.0
Requires-Dist: hf_transfer
Requires-Dist: cut_cross_entropy
Requires-Dist: pillow
Requires-Dist: regex
Requires-Dist: msgspec
Requires-Dist: torch_xla; extra == "tpu"
Provides-Extra: tpu
Requires-Dist: torch_xla>=2.1.0; extra == "tpu"
Requires-Dist: cloud-tpu-client; extra == "tpu"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: isort; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Requires-Dist: pre-commit; extra == "dev"
Provides-Extra: all
Requires-Dist: unsloth_zoo[dev,tpu]; extra == "all"
Dynamic: license-file

<div align="center">

  <h1>üêæ PantheraML Zoo</h1>
  <h3>Production-Ready Multi-GPU/TPU Training Utilities</h3>
  
  <p>
    <strong>Supercharged training with automatic device detection, distributed training, and production monitoring</strong>
  </p>

### PantheraML Zoo - Production Utils for High-Performance Training!

![PantheraML Training](https://i.ibb.co/sJ7RhGG/image-41.png)

</div>

## ‚ú® Finetune for Free

All notebooks are **beginner friendly**! Add your dataset, click "Run All", and you'll get a 2x faster finetuned model which can be exported to GGUF, Ollama, vLLM or uploaded to Hugging Face.

| Unsloth supports | Free Notebooks | Performance | Memory use |
|-----------|---------|--------|----------|
| **Llama 3.2 (3B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing)               | 2x faster | 60% less |
| **Llama 3.1 (8B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)               | 2x faster | 60% less |
| **Phi-3.5 (mini)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing)               | 2x faster | 50% less |
| **Gemma 2 (9B)**      | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)               | 2x faster | 63% less |
| **Mistral Small (22B)** | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/drive/1oCEHcED15DzL8xXGU1VTx5ZfOJM8WY01?usp=sharing)               | 2x faster | 60% less |
| **Ollama**     | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)               | 1.9x faster | 43% less |
| **Mistral v0.3 (7B)**    | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/drive/1_yNCks4BTD5zOnjozppphh5GzMFaMKq_?usp=sharing)               | 2.2x faster | 73% less |
| **ORPO**     | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn?usp=sharing)               | 1.9x faster | 43% less |
| **DPO Zephyr**     | [‚ñ∂Ô∏è Start for free](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 43% less |

- **Kaggle Notebooks** for [Llama 3.1 (8B)](https://www.kaggle.com/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook), [Gemma 2 (9B)](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/), [Mistral (7B)](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
- Run [Llama 3.2 1B 3B notebook](https://colab.research.google.com/drive/1hoHFpf7ROqk_oZHzxQdfPW9yvTxnvItq?usp=sharing) and [Llama 3.2 conversational notebook](https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing)
- Run [Llama 3.1 conversational notebook](https://colab.research.google.com/drive/15OyFkGoCImV9dSsewU1wa2JuKB4-mDE_?usp=sharing) and [Mistral v0.3 ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)
- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining / raw text
- This [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language
- Click [here](https://github.com/unslothai/unsloth/wiki) for detailed documentation for Unsloth.


## üîó Links and Resources
| Type                            | Links                               |
| ------------------------------- | --------------------------------------- |
| üìö **Documentation & Wiki**              | [Read Our Docs](https://docs.unsloth.ai) |
| <img height="14" src="https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|
| üíæ **Installation**               | [unsloth/README.md](https://github.com/unslothai/unsloth/tree/main#-installation-instructions)|
| ü•á **Benchmarking**                   | [Performance Tables](https://github.com/unslothai/unsloth/tree/main#-performance-benchmarking)
| üåê **Released Models**            | [Unsloth Releases](https://huggingface.co/unsloth)|
| ‚úçÔ∏è **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|

## ‚≠ê Key Features
- All kernels written in [OpenAI's Triton](https://openai.com/research/triton) language. **Manual backprop engine**.
- **0% loss in accuracy** - no approximation methods - all exact.
- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.
- Works on **Linux** and **Windows** via WSL.
- Supports 4bit and 16bit QLoRA / LoRA finetuning via [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
- Open source trains 5x faster - see [Unsloth Pro](https://unsloth.ai/) for up to **30x faster training**!
- If you trained a model with ü¶•Unsloth, you can use this cool sticker! &nbsp; <img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png" height="50" align="center" />


## üíæ Installation Instructions

PantheraML Zoo provides production-ready utilities for high-performance training. For stable releases, use `pip install pantheraml_zoo`. We recommend `pip install "pantheraml_zoo @ git+https://github.com/unslothai/unsloth-zoo.git"` for the latest features.

```bash
pip install pantheraml_zoo
```

**Note:** PantheraML Zoo still builds upon [Unsloth](https://github.com/unslothai/unsloth) for core functionality, so install Unsloth as well!

## üöÄ Multi-GPU and TPU Support

PantheraML now supports distributed training across multiple devices:

### Multi-GPU Training

```python
from pantheraml_zoo.training_utils import unsloth_train
from pantheraml_zoo.device_utils import setup_distributed

# Setup distributed training automatically
device_manager = setup_distributed()

# Your existing training code works unchanged
trainer = YourTrainer(model, training_args, train_dataset, ...)
unsloth_train(trainer)
```

**Launch with multiple GPUs:**
```bash
# Use torchrun for multi-GPU training
torchrun --nproc_per_node=4 your_training_script.py

# Or use traditional method
python -m torch.distributed.launch --nproc_per_node=4 your_training_script.py
```

### TPU Training

```python
# TPU is detected automatically - no code changes needed!
# Just run your script normally on a TPU instance
python your_training_script.py
```

### Features:
- ‚úÖ **Automatic device detection** (CUDA, TPU, CPU)
- ‚úÖ **Distributed gradient synchronization**
- ‚úÖ **TPU-optimized training loops**
- ‚úÖ **Mixed precision support** across all devices
- ‚úÖ **Gradient accumulation** with proper scaling
- ‚úÖ **Progress tracking** from main process only
- ‚úÖ **Backward compatibility** with existing code

### Environment Variables:
```bash
# For multi-GPU
export CUDA_VISIBLE_DEVICES=0,1,2,3

# For TPU (on Google Cloud)
export TPU_NAME=your-tpu-name
```

### Device Manager API:
```python
from pantheraml_zoo.device_utils import get_device_manager

dm = get_device_manager()
print(f"Device: {dm.device}")           # Current device
print(f"World Size: {dm.world_size}")   # Number of processes
print(f"Rank: {dm.rank}")               # Current process rank
print(f"Is TPU: {dm.is_tpu}")           # Running on TPU?
print(f"Is Main: {dm.is_main_process}") # Main process?

# Move tensors to device
tensor = dm.to_device(your_tensor)

# Synchronize across processes
dm.barrier()

# All-reduce tensors
reduced_tensor = dm.all_reduce(tensor)
```

## üè≠ Production Features

PantheraML Zoo includes production-ready features for enterprise deployment:

### Production Logging
```python
from pantheraml_zoo import get_logger, setup_production_logging

# Setup structured logging
setup_production_logging(level="INFO", format="json")
logger = get_logger(__name__)

logger.info("Training started", extra={"model": "llama-7b", "batch_size": 16})
```

### Error Handling & Recovery
```python
from pantheraml_zoo import ErrorHandler, with_error_handling

# Automatic checkpointing and recovery
error_handler = ErrorHandler(checkpoint_dir="./checkpoints")

@with_error_handling(error_handler)
def train_model():
    # Your training code here
    pass
```

### Performance Monitoring
```python
from pantheraml_zoo import get_performance_monitor, track_metrics

monitor = get_performance_monitor()

with monitor.training_context():
    # Training automatically tracked
    train_model()
    
# Get comprehensive metrics
metrics = monitor.get_summary()
print(f"Throughput: {metrics['tokens_per_second']} tokens/sec")
```

### Configuration Management
```python
from pantheraml_zoo import load_config, ProductionConfig

# Load from environment variables and config files
config = load_config()

# Or define programmatically
config = ProductionConfig(
    max_sequence_length=4096,
    enable_checkpointing=True,
    checkpoint_frequency=100,
    enable_performance_monitoring=True
)
```


## License

PantheraML Zoo is licensed under the GNU Affero General Public License.
